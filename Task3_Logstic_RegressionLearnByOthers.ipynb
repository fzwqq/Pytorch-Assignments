{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**还未学完**\n",
    "以下为学习代码1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "epoch 1\n",
      "[1/100] Loss: 2.182884, Acc: 0.282604\n",
      "[1/100] Loss: 2.051316, Acc: 0.449740\n",
      "[1/100] Loss: 1.940832, Acc: 0.535660\n",
      "[1/100] Loss: 1.842979, Acc: 0.590391\n",
      "[1/100] Loss: 1.758229, Acc: 0.626271\n",
      "[1/100] Loss: 1.682435, Acc: 0.654201\n",
      "Finish 1 epoch, Loss: 1.665235, Acc: 0.659917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:90: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.218469, Acc: 0.808300\n",
      "Time:6.2 s\n",
      "\n",
      "**********\n",
      "epoch 2\n",
      "[2/100] Loss: 1.200524, Acc: 0.799792\n",
      "[2/100] Loss: 1.163816, Acc: 0.805312\n",
      "[2/100] Loss: 1.131338, Acc: 0.809792\n",
      "[2/100] Loss: 1.101648, Acc: 0.812604\n",
      "[2/100] Loss: 1.075320, Acc: 0.815042\n",
      "[2/100] Loss: 1.052131, Acc: 0.817187\n",
      "Finish 2 epoch, Loss: 1.046778, Acc: 0.817600\n",
      "Test Loss: 0.885615, Acc: 0.839200\n",
      "Time:6.3 s\n",
      "\n",
      "**********\n",
      "epoch 3\n",
      "[3/100] Loss: 0.901032, Acc: 0.826458\n",
      "[3/100] Loss: 0.880881, Acc: 0.831927\n",
      "[3/100] Loss: 0.865625, Acc: 0.834167\n",
      "[3/100] Loss: 0.852300, Acc: 0.836016\n",
      "[3/100] Loss: 0.841573, Acc: 0.836979\n",
      "[3/100] Loss: 0.831034, Acc: 0.837656\n",
      "Finish 3 epoch, Loss: 0.827680, Acc: 0.838167\n",
      "Test Loss: 0.738535, Acc: 0.854500\n",
      "Time:6.2 s\n",
      "\n",
      "**********\n",
      "epoch 4\n",
      "[4/100] Loss: 0.741842, Acc: 0.851354\n",
      "[4/100] Loss: 0.736028, Acc: 0.853073\n",
      "[4/100] Loss: 0.734728, Acc: 0.850069\n",
      "[4/100] Loss: 0.730875, Acc: 0.849219\n",
      "[4/100] Loss: 0.724510, Acc: 0.849854\n",
      "[4/100] Loss: 0.717944, Acc: 0.849826\n",
      "Finish 4 epoch, Loss: 0.717513, Acc: 0.849600\n",
      "Test Loss: 0.655190, Acc: 0.863200\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 5\n",
      "[5/100] Loss: 0.674558, Acc: 0.854792\n",
      "[5/100] Loss: 0.671738, Acc: 0.854740\n",
      "[5/100] Loss: 0.664256, Acc: 0.855035\n",
      "[5/100] Loss: 0.659696, Acc: 0.855052\n",
      "[5/100] Loss: 0.654715, Acc: 0.856375\n",
      "[5/100] Loss: 0.651084, Acc: 0.856510\n",
      "Finish 5 epoch, Loss: 0.650279, Acc: 0.856500\n",
      "Test Loss: 0.600614, Acc: 0.868500\n",
      "Time:5.9 s\n",
      "\n",
      "**********\n",
      "epoch 6\n",
      "[6/100] Loss: 0.618843, Acc: 0.860938\n",
      "[6/100] Loss: 0.618522, Acc: 0.860990\n",
      "[6/100] Loss: 0.615800, Acc: 0.860278\n",
      "[6/100] Loss: 0.610816, Acc: 0.861276\n",
      "[6/100] Loss: 0.608046, Acc: 0.862062\n",
      "[6/100] Loss: 0.605170, Acc: 0.862049\n",
      "Finish 6 epoch, Loss: 0.604292, Acc: 0.862150\n",
      "Test Loss: 0.561924, Acc: 0.873000\n",
      "Time:5.8 s\n",
      "\n",
      "**********\n",
      "epoch 7\n",
      "[7/100] Loss: 0.584077, Acc: 0.863958\n",
      "[7/100] Loss: 0.584262, Acc: 0.863229\n",
      "[7/100] Loss: 0.579005, Acc: 0.864514\n",
      "[7/100] Loss: 0.578104, Acc: 0.865313\n",
      "[7/100] Loss: 0.574461, Acc: 0.865521\n",
      "[7/100] Loss: 0.571396, Acc: 0.865920\n",
      "Finish 7 epoch, Loss: 0.570552, Acc: 0.866100\n",
      "Test Loss: 0.532828, Acc: 0.876800\n",
      "Time:5.8 s\n",
      "\n",
      "**********\n",
      "epoch 8\n",
      "[8/100] Loss: 0.553745, Acc: 0.869792\n",
      "[8/100] Loss: 0.548278, Acc: 0.870000\n",
      "[8/100] Loss: 0.550026, Acc: 0.868021\n",
      "[8/100] Loss: 0.549932, Acc: 0.868464\n",
      "[8/100] Loss: 0.547180, Acc: 0.869313\n",
      "[8/100] Loss: 0.544998, Acc: 0.869670\n",
      "Finish 8 epoch, Loss: 0.544522, Acc: 0.869350\n",
      "Test Loss: 0.510092, Acc: 0.879800\n",
      "Time:5.9 s\n",
      "\n",
      "**********\n",
      "epoch 9\n",
      "[9/100] Loss: 0.521211, Acc: 0.875938\n",
      "[9/100] Loss: 0.518924, Acc: 0.876042\n",
      "[9/100] Loss: 0.517835, Acc: 0.876042\n",
      "[9/100] Loss: 0.520204, Acc: 0.874583\n",
      "[9/100] Loss: 0.521458, Acc: 0.873563\n",
      "[9/100] Loss: 0.523747, Acc: 0.872188\n",
      "Finish 9 epoch, Loss: 0.523722, Acc: 0.872333\n",
      "Test Loss: 0.491747, Acc: 0.882200\n",
      "Time:5.8 s\n",
      "\n",
      "**********\n",
      "epoch 10\n",
      "[10/100] Loss: 0.512148, Acc: 0.873125\n",
      "[10/100] Loss: 0.511562, Acc: 0.873594\n",
      "[10/100] Loss: 0.514434, Acc: 0.872604\n",
      "[10/100] Loss: 0.511237, Acc: 0.873229\n",
      "[10/100] Loss: 0.509152, Acc: 0.873750\n",
      "[10/100] Loss: 0.507186, Acc: 0.874236\n",
      "Finish 10 epoch, Loss: 0.506615, Acc: 0.874450\n",
      "Test Loss: 0.476265, Acc: 0.883600\n",
      "Time:6.3 s\n",
      "\n",
      "**********\n",
      "epoch 11\n",
      "[11/100] Loss: 0.496333, Acc: 0.874167\n",
      "[11/100] Loss: 0.496234, Acc: 0.875313\n",
      "[11/100] Loss: 0.495595, Acc: 0.875625\n",
      "[11/100] Loss: 0.494751, Acc: 0.876172\n",
      "[11/100] Loss: 0.492402, Acc: 0.877146\n",
      "[11/100] Loss: 0.492175, Acc: 0.876927\n",
      "Finish 11 epoch, Loss: 0.492246, Acc: 0.877050\n",
      "Test Loss: 0.463206, Acc: 0.884700\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 12\n",
      "[12/100] Loss: 0.481080, Acc: 0.879167\n",
      "[12/100] Loss: 0.478960, Acc: 0.879479\n",
      "[12/100] Loss: 0.481576, Acc: 0.878472\n",
      "[12/100] Loss: 0.482267, Acc: 0.878125\n",
      "[12/100] Loss: 0.480596, Acc: 0.878958\n",
      "[12/100] Loss: 0.480422, Acc: 0.878472\n",
      "Finish 12 epoch, Loss: 0.479963, Acc: 0.878733\n",
      "Test Loss: 0.452086, Acc: 0.886300\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 13\n",
      "[13/100] Loss: 0.466045, Acc: 0.885000\n",
      "[13/100] Loss: 0.467841, Acc: 0.883385\n",
      "[13/100] Loss: 0.472348, Acc: 0.881250\n",
      "[13/100] Loss: 0.472163, Acc: 0.880391\n",
      "[13/100] Loss: 0.469692, Acc: 0.880958\n",
      "[13/100] Loss: 0.469662, Acc: 0.880625\n",
      "Finish 13 epoch, Loss: 0.469293, Acc: 0.880667\n",
      "Test Loss: 0.442278, Acc: 0.887800\n",
      "Time:6.5 s\n",
      "\n",
      "**********\n",
      "epoch 14\n",
      "[14/100] Loss: 0.473306, Acc: 0.881250\n",
      "[14/100] Loss: 0.465592, Acc: 0.881771\n",
      "[14/100] Loss: 0.463015, Acc: 0.882847\n",
      "[14/100] Loss: 0.461593, Acc: 0.882109\n",
      "[14/100] Loss: 0.460346, Acc: 0.882625\n",
      "[14/100] Loss: 0.459857, Acc: 0.882413\n",
      "Finish 14 epoch, Loss: 0.459935, Acc: 0.882133\n",
      "Test Loss: 0.433674, Acc: 0.889000\n",
      "Time:6.6 s\n",
      "\n",
      "**********\n",
      "epoch 15\n",
      "[15/100] Loss: 0.454077, Acc: 0.887396\n",
      "[15/100] Loss: 0.458084, Acc: 0.882031\n",
      "[15/100] Loss: 0.457297, Acc: 0.882917\n",
      "[15/100] Loss: 0.457031, Acc: 0.882474\n",
      "[15/100] Loss: 0.453225, Acc: 0.883792\n",
      "[15/100] Loss: 0.451536, Acc: 0.883924\n",
      "Finish 15 epoch, Loss: 0.451622, Acc: 0.884017\n",
      "Test Loss: 0.426038, Acc: 0.890200\n",
      "Time:6.7 s\n",
      "\n",
      "**********\n",
      "epoch 16\n",
      "[16/100] Loss: 0.455840, Acc: 0.883125\n",
      "[16/100] Loss: 0.449519, Acc: 0.884219\n",
      "[16/100] Loss: 0.445082, Acc: 0.885000\n",
      "[16/100] Loss: 0.444917, Acc: 0.885234\n",
      "[16/100] Loss: 0.444179, Acc: 0.885708\n",
      "[16/100] Loss: 0.443922, Acc: 0.885139\n",
      "Finish 16 epoch, Loss: 0.444210, Acc: 0.884917\n",
      "Test Loss: 0.419207, Acc: 0.892300\n",
      "Time:6.4 s\n",
      "\n",
      "**********\n",
      "epoch 17\n",
      "[17/100] Loss: 0.445472, Acc: 0.888646\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import time\n",
    "# 定义超参数\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epoches = 100\n",
    "\n",
    "# 下载训练集 MNIST 手写数字训练集\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# 定义 Logistic Regression 模型\n",
    "class Logstic_Regression(nn.Module):\n",
    "    def __init__(self, in_dim, n_class):\n",
    "        super(Logstic_Regression, self).__init__()\n",
    "        self.logstic = nn.Linear(in_dim, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.logstic(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Logstic_Regression(28 * 28, 10)  # 图片大小是28x28\n",
    "use_gpu = torch.cuda.is_available()  # 判断是否有GPU加速\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "# 定义loss和optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(num_epoches):\n",
    "    print('*' * 10)\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    since = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in enumerate(train_loader, 1):\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)  # 将图片展开成 28x28\n",
    "        if use_gpu:\n",
    "            img = Variable(img).cuda()\n",
    "            label = Variable(label).cuda()\n",
    "        else:\n",
    "            img = Variable(img)\n",
    "            label = Variable(label)\n",
    "        # 向前传播\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        running_loss += loss.item() * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        running_acc += num_correct.item()\n",
    "        # 向后传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 300 == 0:\n",
    "            print('[{}/{}] Loss: {:.6f}, Acc: {:.6f}'.format(\n",
    "                epoch + 1, num_epoches, running_loss / (batch_size * i),\n",
    "                running_acc / (batch_size * i)))\n",
    "    print('Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\n",
    "        epoch + 1, running_loss / (len(train_dataset)), running_acc / (len(\n",
    "            train_dataset))))\n",
    "    model.eval()\n",
    "    eval_loss = 0.\n",
    "    eval_acc = 0.\n",
    "    for data in test_loader:\n",
    "        img, label = data\n",
    "        img = img.view(img.size(0), -1)\n",
    "        if use_gpu:\n",
    "            img = Variable(img, volatile=True).cuda()\n",
    "            label = Variable(label, volatile=True).cuda()\n",
    "        else:\n",
    "            img = Variable(img, volatile=True)\n",
    "            label = Variable(label, volatile=True)\n",
    "        out = model(img)\n",
    "        loss = criterion(out, label)\n",
    "        eval_loss += loss.item() * label.size(0)\n",
    "        _, pred = torch.max(out, 1)\n",
    "        num_correct = (pred == label).sum()\n",
    "        eval_acc += num_correct.item()\n",
    "    print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "        test_dataset)), eval_acc / (len(test_dataset))))\n",
    "    print('Time:{:.1f} s'.format(time.time() - since))\n",
    "    print()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), './logstic.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习代码-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2)\n",
    "x_data = Variable(torch.Tensor([[1.0],[2.0],[3.0],[4.0]]))\n",
    "y_data = Variable(torch.Tensor([[0.0],[0.0],[1.],[1.]])\n",
    "w * x_data \n",
    "1 /(1 + torch.exp(- w *x_data))\n",
    "                  \n",
    "w = Variable( )\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
